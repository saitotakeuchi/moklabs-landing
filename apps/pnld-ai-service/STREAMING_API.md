# Streaming Chat API Documentation

## Overview

The streaming chat endpoint enables real-time streaming of AI responses using Server-Sent Events (SSE). This provides a better user experience by showing responses as they are generated rather than waiting for the complete response.

## Endpoint

**POST** `/api/v1/chat/stream`

## Request

### Headers
```
Content-Type: application/json
Accept: text/event-stream
```

### Body
```json
{
  "message": "Your question here",
  "conversation_id": "optional-conversation-id",
  "edital_id": "optional-edital-id",
  "max_tokens": 1000,
  "temperature": 0.7
}
```

### Parameters

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `message` | string | Yes | - | The user's question or message |
| `conversation_id` | string | No | null | ID to continue an existing conversation |
| `edital_id` | string | No | null | Scope the search to a specific edital |
| `max_tokens` | integer | No | 1000 | Maximum tokens in the response (1-4000) |
| `temperature` | float | No | 0.7 | LLM temperature parameter (0.0-2.0) |

## Response

The endpoint returns a stream of Server-Sent Events (SSE) with different event types.

### Event Types

#### 1. `metadata` Event
Sent first with conversation metadata.

```
event: metadata
data: {"conversation_id": "uuid-here"}
```

**Fields:**
- `conversation_id` (string): The conversation identifier for this chat session

#### 2. `sources` Event
Sent after document retrieval with source citations.

```
event: sources
data: [
  {
    "document_id": "doc-uuid",
    "title": "Document Title",
    "content_excerpt": "Relevant excerpt...",
    "relevance_score": 0.85,
    "page_number": 5,
    "chunk_index": 2,
    "edital_id": "edital-uuid"
  }
]
```

**Fields:**
- `document_id` (string): Unique document identifier
- `title` (string): Document title
- `content_excerpt` (string): First 200 characters of relevant content
- `relevance_score` (float): Similarity score (0.0-1.0)
- `page_number` (integer|null): Page number where content was found
- `chunk_index` (integer|null): Chunk index within the page
- `edital_id` (string|null): Associated edital identifier

#### 3. `token` Event
Sent for each token as it's generated by the LLM.

```
event: token
data: {"content": "token"}
```

**Fields:**
- `content` (string): A single token or part of the response text

#### 4. `done` Event
Sent when streaming is complete.

```
event: done
data: {"conversation_id": "uuid-here"}
```

**Fields:**
- `conversation_id` (string): The conversation identifier

#### 5. `error` Event
Sent if an error occurs during streaming.

```
event: error
data: {
  "error": "Error message",
  "conversation_id": "uuid-here"
}
```

**Fields:**
- `error` (string): Error description
- `conversation_id` (string|null): The conversation identifier if available

## Event Flow

```
1. metadata   → Conversation created/retrieved
2. sources    → Relevant documents found
3. token      → First token of response
4. token      → Second token
5. token      → Third token
   ...
N. done       → Stream complete
```

## Example Usage

### Using cURL

```bash
curl -N -X POST http://localhost:8000/api/v1/chat/stream \
  -H "Content-Type: application/json" \
  -H "Accept: text/event-stream" \
  -d '{
    "message": "What is PNLD?",
    "max_tokens": 500,
    "temperature": 0.7
  }'
```

### Using Python

```python
import requests
import json

url = "http://localhost:8000/api/v1/chat/stream"
payload = {
    "message": "What is PNLD?",
    "max_tokens": 500,
    "temperature": 0.7
}

response = requests.post(
    url,
    json=payload,
    stream=True,
    headers={"Accept": "text/event-stream"}
)

for line in response.iter_lines():
    if line:
        line = line.decode('utf-8')
        if line.startswith('event:'):
            event_type = line.split(':', 1)[1].strip()
        elif line.startswith('data:'):
            data = json.loads(line.split(':', 1)[1].strip())
            print(f"{event_type}: {data}")
```

### Using JavaScript (Browser)

```javascript
const eventSource = new EventSource('/api/v1/chat/stream', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({
    message: 'What is PNLD?',
    max_tokens: 500,
    temperature: 0.7
  })
});

eventSource.addEventListener('metadata', (e) => {
  const data = JSON.parse(e.data);
  console.log('Conversation ID:', data.conversation_id);
});

eventSource.addEventListener('sources', (e) => {
  const sources = JSON.parse(e.data);
  console.log('Sources:', sources);
});

eventSource.addEventListener('token', (e) => {
  const data = JSON.parse(e.data);
  process.stdout.write(data.content); // Append to UI
});

eventSource.addEventListener('done', (e) => {
  const data = JSON.parse(e.data);
  console.log('Stream complete!');
  eventSource.close();
});

eventSource.addEventListener('error', (e) => {
  const data = JSON.parse(e.data);
  console.error('Error:', data.error);
  eventSource.close();
});
```

## Testing

A test script is provided at `test_streaming.py`:

```bash
# Make sure the service is running
poetry run python -m app.main

# In another terminal, run the test
python test_streaming.py
```

## Implementation Details

### Backend Architecture

1. **Endpoint Handler** (`app/api/v1/chat.py:chat_stream`)
   - Creates/retrieves conversation
   - Manages event generation
   - Stores messages in database
   - Handles errors gracefully

2. **RAG Pipeline** (`app/services/rag.py:generate_rag_response_stream`)
   - Retrieves relevant documents from vector store
   - Yields sources immediately after retrieval
   - Streams LLM response token by token
   - Signals completion

3. **LLM Streaming** (`app/services/rag.py:generate_llm_response_stream`)
   - Uses OpenAI's streaming API
   - Yields tokens as they arrive
   - Maintains conversation context

### Database Operations

The endpoint performs the following database operations:

1. **Create/Retrieve Conversation** - Creates a new conversation or uses existing one
2. **Store User Message** - Saves the user's message before streaming
3. **Store Assistant Response** - Saves complete response after streaming completes

### Error Handling

Errors are gracefully handled and sent as `error` events:
- Database connection failures
- Vector search errors
- OpenAI API errors
- General exceptions

## Differences from Non-Streaming Endpoint

| Feature | `/api/v1/chat` | `/api/v1/chat/stream` |
|---------|----------------|------------------------|
| Response Type | JSON | Server-Sent Events |
| Response Time | Waits for complete response | Immediate streaming |
| Use Case | Simple integration | Real-time UX |
| Content-Type | `application/json` | `text/event-stream` |
| Error Handling | HTTP error codes | `error` events |

## Best Practices

1. **Client-Side Buffering**: Accumulate tokens on the client to build the complete message
2. **Error Handling**: Always listen for `error` events and handle gracefully
3. **Connection Management**: Close the SSE connection when `done` event is received
4. **Timeout Handling**: Implement client-side timeouts for long-running requests
5. **Retry Logic**: Implement exponential backoff for connection failures

## Notes

- The non-streaming endpoint (`/api/v1/chat`) remains fully functional
- Both endpoints store conversation history in the same database tables
- Source citations are sent before the response text begins
- The complete response is stored in the database after streaming completes
